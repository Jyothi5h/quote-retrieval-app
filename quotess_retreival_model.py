# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bj8Uzgnbp5hOWwClZkJF3g8v_1KnwRyk

## Importing Necessary Libraries
"""

import pandas as pd
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import T5Tokenizer, T5ForConditionalGeneration
from datasets import Dataset
from evaluate import load
from sentence_transformers import InputExample, losses
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

"""# Data Pre-Processing

"""

df = pd.read_json("hf://datasets/Abirate/english_quotes/quotes.jsonl", lines=True)

df.head()

df.isnull().sum()

# Clean data
df = df.dropna(subset=['quote', 'author', 'tags'])
df['quote'] = df['quote'].str.lower().str.strip()
df['author'] = df['author'].str.lower().str.strip()
df['tags'] = df['tags'].apply(lambda x: [t.lower() for t in x])

# Save cleaned dataset
df.to_csv("cleaned_quotes.csv", index=False)

"""# Loading and training the BGE model"""

# Load BGE model
model = SentenceTransformer("BAAI/bge-base-en-v1.5")

# Prepare training examples
examples = []
for i, row in df.iterrows():
    query = f"query: quotes about {' '.join(row['tags'])} by {row['author']}"
    passage = "passage: " + row['quote']
    examples.append(InputExample(texts=[query, passage]))

# Creating DataLoader
train_dataloader = DataLoader(examples, shuffle=True, batch_size=16)
train_loss = losses.MultipleNegativesRankingLoss(model)

# Training the model
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)

#saving the model
model.save("fine_tuned_quote_model")

"""# Response generation"""

# Loading the saved model
model = SentenceTransformer("/content/fine_tuned_bge_quote_model")

# Encoding the corpus with "passage: " prefix
corpus = ["passage: " + quote for quote in df['quote'].tolist()]
corpus_embeddings = model.encode(corpus, convert_to_tensor=False)

# FAISS index
dimension = corpus_embeddings[0].shape[0]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(corpus_embeddings))

# Saving the index and metadata
faiss.write_index(index, "quotes_index.faiss")
df.to_csv("quote_metadata.csv", index=False)

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
flan_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
flan_pipe = pipeline("text2text-generation", model=flan_model, tokenizer=tokenizer)

def retrieve_and_generate(query):
    query_embedding = model.encode(["query: " + query])[0]
    D, I = index.search(np.array([query_embedding]), k=5)
    top_quotes = [df['quote'].iloc[i] for i in I[0]]
    top_authors = [df['author'].iloc[i] for i in I[0]]
    top_tags = [df['tags'].iloc[i] for i in I[0]]

    prompt = f"Using the following quotes, answer the query: {query}\n"
    for quote, author in zip(top_quotes, top_authors):
        prompt += f"- {quote} ({author})\n"

    response = flan_pipe(prompt, max_new_tokens=100)[0]['generated_text']

    return {
        "query": query,
        "quotes": top_quotes,
        "authors": top_authors,
        "tags": top_tags,
        "response": response
    }

"""# RAG Evaluation"""

#RAG Evaluation (Rouge + BLEU)

from evaluate import load

rouge = load("rouge")
bleu = load("bleu")

def rag_evaluation():
    result = retrieve_and_generate("Quotes about courage by women authors")

    data = {
        "question": [result['query']],
        "contexts": [result['quotes']],
        "answer": [result['response']]
    }

    rouge_score = rouge.compute(predictions=data["answer"], references=["\n".join(data["contexts"][0])])
    bleu_score = bleu.compute(predictions=[data["answer"][0]], references=["\n".join(data["contexts"][0])])
    print("ROUGE-L:", rouge_score['rougeL'])
    print("BLEU:", bleu_score['bleu'])

if __name__ == "__main__":
    rag_evaluation()

